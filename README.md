# editor_ia

**editor_ia** is a local tool designed to revise and polish Chinese-to-English webnovel translations — especially those generated by machine (MTL) or amateur efforts. It focuses on **clarity, fluency, and readability**, without inventing or altering meaning.

---

## 🧠 The Model

Currently uses [`NousResearch/Hermes-2-Pro-Mistral-7B`](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B) or similar variants (like `OpenHermes`) via Hugging Face, because it:
- runs efficiently on local GPU (CUDA-enabled),
- respects structured prompts with `<|im_start|>` tags,
- avoids hallucinations or censorship,
- provides clean, human-like revisions.

---

## 📚 Why This Exists

Most MTLs or quick fan translations suffer from:
- poor grammar and unnatural flow,
- inconsistent paragraph structure,
- filtered or mangled meaning in key terms.

This tool:
- preserves tone, pacing, idioms, and special terms (e.g., *cultivation*, *immortal*, *dao*),
- never reassigns dialogue or adds content,
- only improves grammar, punctuation, and sentence clarity.

---

## 🔨 Key Features

### ✅ Block-Based Segmentation
Each chapter is broken into natural **blocks** (typically 3–7 lines), ending on punctuation. This ensures LLMs get context-rich but bounded prompts.

### ✅ Strict Prompt Control
The system uses `<|im_start|>`/`<|im_end|>` formatting to define:
- the role of system, user, and assistant,
- what editing is allowed (clarity only, no invention),
- which terms must be preserved.

### 🧠 Smart Fallback Handling
If the model fails to revise a block:
1. A second attempt is made with a higher temperature.
2. If it still fails, the original block is kept.
3. Final output integrity is verified — no empty blocks are allowed.

This guarantees stable revision across hundreds of blocks.

### 🧼 Automatic Cleanup
After generation, each block is cleaned via regex to remove:
- chat prompt artifacts (e.g. `<|im_x|>`, `<start>`, `<note>`),
- LLM summaries like “The end.” or “No further edits needed.”,
- empty or repeated Markdown code blocks (like ` ``` `).

This ensures clean output in `.docx`.

### 🧾 Detailed Logging
Each run creates a log in `dados/logs/` with per-chapter stats:
- Number of blocks,
- Input/output token count,
- Duration,
- Fallback stats:
  - Revised on first try
  - Revised on second try
  - Kept as original
  - Recovered manually after final cleanup

### 🖥️ Live Terminal Feedback
Terminal shows chapter-by-chapter status and timing, e.g.:

[📖] 3/10: Chapter 2003: Surging Will
[✅] Finalizado: Chapter 2003: Surging Will (1m 55s)

## 🚀 How to Use

1. Place your `.docx` file(s) in `dados/entrada/`.
2. Run `app.py`, or call `revisar_docx_otimizado()` directly.
3. The reviewed version will appear in `dados/saida/`.
4. Logs are stored in `dados/logs/`.

You can configure:
- the model, temperature, and prompt via `utils/config.py`.

---

## 📁 Project Structure

editor_ia/
├── app.py
├── dados/
│ ├── entrada/
│ ├── saida/
│ └── logs/
├── modelo/
│ └── carregador.py
├── processamento/
│ ├── segmentador.py
│ ├── revisor_llm.py
├── editor/
│ └── editor_docx.py
├── utils/
│ ├── config.py
│ └── logger.py

yaml
Copiar
Editar

---

## ⏱️ Performance

On an RTX 4080:
- ~2,500 tokens per chapter
- 2–3 minutes per chapter

Performance depends on number of chapters and block segmentation.

---

## 🔒 Disclaimer

This is a personal tool for cleaning up legally acquired material.  
No content is hosted, redistributed, or republished.

---

## ✍️ Built by

Developed by **Roman Brocki** with efficiency, modularity, and quality in mind — focused on fast, GPU-accelerated processing of long-form fiction with high fidelity to the source.