# editor_ia

**editor_ia** is a personal project built to revise and improve the readability of Chinese-to-English webnovel translations. The goal here isnâ€™t to rewrite chapters or republish anything â€” itâ€™s just to make amateur translated or machine translated novels more readable, emotional, and smooth without losing their original tone, names, or structure.

This repo exists because current LLM pipelines arenâ€™t great at this task out of the box â€” especially when you need something that:
- doesnâ€™t censor plot-relevant content,
- runs on GPU (CUDA),
- is consistent with style and output,
- and doesn't hallucinate or try to "explain" the story.

---

## ğŸ§  The Model

We use [teknium/OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B), which so far has been one of the best uncensored models that:
- runs reliably on a local GPU,
- produces stable, high-quality outputs,
- respects the prompt structure (when properly prepared).

Finding a balance between performance, weight and being uncensored was a headache. Many models either filtered entire blocks, refused to edit â€œsensitiveâ€ text, or added commentary. This one doesnâ€™t â€” which is why it was chosen.

---

## ğŸ“š Why This Exists

Weâ€™re not trying to â€œrefineâ€ a translation into literature â€” just clean it up enough to read fluently. This means:
- No name changes
- No removing of emotional exaggeration
- No reassigning who said what
- No turning "sword dao" into â€œideological martial trainingâ€ or some nonsense

Only **fixing what breaks immersion**, and leaving the rest as is.

---

## ğŸ”¨ Key Features and Design Decisions

### âœ… Block Segmentation
LLMs donâ€™t deal well with giant blobs of text â€” and full chapters break prompt limits fast. So we split each chapter into **smaller, natural-sounding blocks** using punctuation and line count as boundaries.

This keeps prompts short and coherent, and keeps the model from losing the thread.

### âœ… re.sub Cleanup
Unfortunately, even good models sometimes act like chatbots.  
That means:
- echoing the prompt,
- re-injecting system messages,
- or adding `user:` and `assistant:` tags.

All of that is stripped out with carefully tuned `re.sub()` calls that we tested against real outputs. This wasnâ€™t overkill â€” it was necessary.

### âœ… Modular Structure
We split the code into clean modules:

- `modelo/`: loads and preps the model/tokenizer

- `processamento/`: handles block segmentation and interaction with the LLM.

- `editor/`: coordinates the docx read/segment/rewrite/save flow

- `dados/entrada/` and `dados/saida/`: clear separation of raw vs processed files

We didn't invent extra functions unless absolutely needed â€” just separated logic for clarity.

---

## ğŸš€ How to Use

1. Drop your `.docx` file (a chapter batch) into `dados/entrada/`. 
2. You can adjust the model's temperature, ranging from 0.6 to 0.8 â€” increasing it makes the editor more likely to introduce creative changes.
3. Run `app.py`
4. The revised version will show up in `dados/saida/`, with `_revisado` added to the filename

âš ï¸ On first run, the Hugging Face model (`teknium/OpenHermes-2.5-Mistral-7B`) will be downloaded automatically.  
You may need to log in to your Hugging Face account if you haven't configured access before.

Youâ€™ll get live progress updates, batch processing logs, and a total time summary at the end.

---

## ğŸ“ Project Structure

```
editor_ia/
â”œâ”€â”€ app.py                  # Run this to start the revision process
â”œâ”€â”€ dados/
â”‚   â”œâ”€â”€ entrada/            # Drop your original .docx files here
â”‚   â””â”€â”€ saida/              # Revised files appear here
â”œâ”€â”€ modelo/
â”‚   â””â”€â”€ carregador.py       # Loads tokenizer and model
â”œâ”€â”€ processamento/
â”‚   â”œâ”€â”€ segmentador.py      # Splits text into manageable blocks
â”‚   â”œâ”€â”€ revisor_llm.py      # Sends prompts to the model and processes response
â”‚   â””â”€â”€ limpador.py         # (optional, for cleanup logic)
â”œâ”€â”€ editor/
â”‚   â””â”€â”€ editor_docx.py      # Coordinates docx loading and saving
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ constantes.py       # (optional, for shared config/prompt storage)
```


---

## â±ï¸ Performance

On an RTX 4080 GPU, the processing time is approximately **3 minutes for every 2200 words**. This can vary depending on:
- how the text gets tokenized,
- how many lines end up in each block,
- and how consistently the model adheres to the prompt instructions.

---

## ğŸ”’ Usage Disclaimer

This is strictly a **personal-use** tool for improving private readability of purchased or archived content.  
We donâ€™t host, share, or re-upload any original or edited material.  
If you're using this, you're expected to respect copyright and usage rights.

---

## âœï¸ Built by

Created by **Roman Brocki**, mixing personal expertise with the assistance of **ChatGPT-4o**  
to refine structure, modularize components, and optimize model interaction for speed and reliability.

This project came out of necessity â€” tested against real chapters, and tuned until it worked the way it should.
